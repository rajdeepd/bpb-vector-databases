{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537b2b55-719a-4ae9-afc1-468d689d0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709aaf7b-eb71-49b7-bbd4-bbbcb72cf97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"llama3.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bd06ac-5ddd-4bdd-9296-e13aec67ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = \"The meaning of life is 42\"\n",
    "#vector = embed.embed_query(input_text)\n",
    "#print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfb3790-1626-4d8c-a7d1-ab1682b69fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader \n",
    "from langchain.indexes import VectorstoreIndexCreator \n",
    "#loader = WebBaseLoader(\"https://www.promptingguide.ai/techniques/rag\") \n",
    "#index = VectorstoreIndexCreator().from_loaders([loader]) \n",
    "#index.query(\"What is RAG?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2ac1bb-ab84-4f88-b54e-62472ef933da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_multiple_pages = WebBaseLoader([\"https://www.promptingguide.ai/techniques/rag\", \n",
    "                                       \"https://python.langchain.com/v0.2/docs/tutorials/rag/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e44bf7e-56d2-45f7-89e2-bce68f5deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader_multiple_pages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d73baa0-2f21-457b-b6e7-8407928fbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_text = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f137c74-028a-4c8d-ab6c-b9d269083c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_texts = []\n",
    "for d in docs:\n",
    "    tmp_string_text = d.page_content\n",
    "    string_texts.append(tmp_string_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed3136e-e19e-4688-9447-a491d3da0ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_texts[1][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d4518a7-41c1-48f9-b730-c775726ddcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[-0.0038624534, -0.020101622, 0.0038441762]\n"
     ]
    }
   ],
   "source": [
    "#input_texts = [\"Document 1...\", \"Document 2...\"]\n",
    "vectors = embed.embed_documents(string_texts)\n",
    "print(len(vectors))\n",
    "# The first 3 coordinates for the first vector\n",
    "print(vectors[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36fc930b-dc35-4651-847a-177bd0330ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuideðŸŽ“ Prompt Engineering CourseðŸŽ“ Prompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingMeta PromptingSelf-ConsistencyGenerate Knowledge PromptingPrompt ChainingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingProgram-Aided Language ModelsReActReflexionMultimodal CoTGraph PromptingGuidesOptimizing PromptsApplicationsFine-tuning GPT-4oFunction CallingContext Caching with LLMsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionPrompt HubClassificationSentiment ClassificationFew-Shot Sentiment ClassificationCodingGenerate Code SnippetGenerate MySQL QueryDraw TiKZ DiagramCreativityRhymesInfinite PrimesInterdisciplinaryInventing New WordsEvaluationEvaluate Plato\\'s DialogueInformation ExtractionExtract Model NamesImage GenerationDraw a Person Using AlphabetMathematicsEvaluating Composite FunctionsAdding Odd NumbersQuestion AnsweringClosed Domain Question AnsweringOpen Domain Question AnsweringScience Question AnsweringReasoningIndirect ReasoningPhysical ReasoningText SummarizationExplain A ConceptTruthfulnessHallucination IdentificationAdversarial PromptingPrompt InjectionPrompt LeakingJailbreakingModelsChatGPTClaude 3Code LlamaFlanGeminiGemini AdvancedGemini 1.5 ProGemmaGPT-4Grok-1LLaMALlama 3Mistral 7BMistral LargeMixtralMixtral 8x22BOLMoPhi-2SoraLLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesLLM Research FindingsLLM AgentsRAG for LLMsLLM ReasoningRAG FaithfulnessLLM In-Context RecallRAG Reduces HallucinationSynthetic DataThoughtSculptInfini-AttentionLM-Guided CoTTrustworthiness in LLMsLLM TokenizationWhat is Groq?PapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightOn This PageRAG Use Case: Generating Friendly ML Paper TitlesReferencesQuestion? Give us feedback â†’ (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)\\n\\nGeneral-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don\\'t require additional background knowledge.\\nFor more complex and knowledge-intensive tasks, it\\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of \"hallucination\".\\nMeta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.\\nRAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs\\'s parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.\\nLewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:\\n\\nImage Source: Lewis et el. (2021) (opens in a new tab)\\nRAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.\\nThis shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.\\nMore recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.\\nRAG Use Case: Generating Friendly ML Paper Titles\\nBelow, we have prepared a notebook tutorial showcasing the use of open-source LLMs to build a RAG system for generating short and concise machine learning paper titles:\\nGetting Started with RAG\\nðŸŽ“Want to learn more about RAG? Check out our new cohort-based course (opens in a new tab). Use promo code MAVENAI20 for a 20% discount.\\nReferences\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey (opens in a new tab) (Dec 2023)\\nRetrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models (opens in a new tab) (Sep 2020)\\nTree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright Â© 2024 DAIR.AI'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    string_texts,\n",
    "    embedding=embed,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is RAG?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0dd0c-a987-4618-bf13-cd46d6e31e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-aug-2024-kernel",
   "language": "python",
   "name": "my-python3-aug-2024-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
