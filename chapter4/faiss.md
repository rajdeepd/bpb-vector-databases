# What Is Faiss (Facebook AI Similarity Search)?

The emergence of deep learning has transformed how complex data is stored and retrieved, largely through the use of embeddings—vector representations generated by neural networks. These embeddings map input media (such as text, images, and audio) into a vector space where proximity corresponds to semantic similarity. They serve as compact, reusable intermediate representations for tasks like classification, recommendation, and multimodal analysis.

Embeddings are increasingly used for similarity search, especially in scenarios where retraining large neural networks is impractical. Instead, k-nearest-neighbor methods can more efficiently incorporate new data. This trend has led to a growing reliance on vector-capable database systems that integrate traditional DBMS functionalities with Approximate Nearest Neighbor Search (ANNS) algorithms.



## What Is Faiss?

Faiss (Facebook AI Similarity Search) is a specialized library developed to address this need. It provides a broad set of tools for high-speed similarity search in dense vector spaces. Unlike some other systems, Faiss does not perform feature extraction or operate as a full-fledged database. Rather, it focuses exclusively on indexing and searching embeddings with support for multiple indexing strategies, distance metrics, and CPU/GPU processing.

The library’s core is a flexible indexing structure that stores database vectors and returns the closest matches to a given query vector. Options include retrieving a fixed number of neighbors, filtering by distance thresholds, or handling large batches of queries in parallel.

Since its open-source release in 2017, Faiss has become widely adopted, with over 30,000 GitHub stars, 3 million downloads, and thousands of citations. Major vector database companies like Zilliz and Pinecone rely on Faiss or have integrated similar algorithms. The paper details Faiss's design principles, trade-offs in similarity search, and its applications in large-scale indexing and content retrieval.

## Core functionality
Faiss makes nearest-neighbor searches fast by indexing vectors using sophisticated algorithms like k-means clustering and product quantization. These methods help Faiss organize and retrieve vectors efficiently, ensuring similarity searches are quick and accurate. Here's a closer look at the indexing algorithms:

**K-means clustering**
This algorithm breaks the data into clusters, which helps narrow down the search space by focusing on the most relevant clusters during queries.
**Product quantization (PQ)**: PQ compresses vectors into shorter codes, reducing memory usage significantly and speeding up the search without a big drop in accuracy.
**Optimized product quantization (OPQ)**: An enhanced version of PQ, OPQ rotates the data to better fit the quantization grid, improving the accuracy of the compressed vectors.
Flexibility
Faiss is pretty EFFICNET when it comes to measuring similarity between vectors, offering a variety of distance metrics to choose from. The main ones are:

**Euclidean distance:** This measures the straight-line distance between two points, which is great when you care about the geometric similarity of vectors.
**Cosine similarity:** This measure calculates the cosine of the angle between two vectors, focusing more on their orientation than their size. It's especially handy for text analysis where the direction matters more than the length.
These options allow you to pick the metric that best fits your data and application needs.

Faiss can run on both CPUs and GPUs, using modern hardware to speed up the search process. Faiss is designed for various computing platforms, from personal computers to high-performance computing clusters. It smoothly transitions between CPU and GPU indices, and its Python interface works well with C++ indices, making it easy to switch from testing to deployment. This multi-platform support makes sure that Faiss can be efficiently used in various computing environments, optimizing performance and resource use.

### Features of Faiss
Faiss is a standout tool for similarity search, packed with features designed to handle large and diverse datasets effectively. Here’s a closer look at some of the core capabilities that make it a powerful asset for data-intensive tasks.

Scalability
Faiss is designed to manage datasets from millions to billions of vectors, which is perfect for applications like large recommendation systems or massive image and video databases. It uses advanced techniques like inverted file systems and hierarchical navigable small world (HNSW) graphs to keep things efficient even with extensive datasets.

Speed
Faiss is fast due to its optimized algorithms and data structures. It uses k-means clustering, product quantization, and optimized brute-force searches to speed things up. If you’re using a GPU, Faiss can be up to 20 times faster on newer Pascal-class hardware compared to its CPU versions. This speed is important for real-time applications where you might need quick responses.

Accuracy
Faiss gives you flexibility in accuracy, balancing speed and precision based on what you need. You can fine-tune it for highly accurate searches or go for quicker, less precise results. There are different indexing methods and parameters to choose from, and you can measure performance with metrics like 1-recall@1 and 10-intersection to see how well it’s doing compared to a brute-force approach.

Versatility
Faiss can handle different types of data by converting them into vector representations. This means you can use it for images, text, audio, and more, making it useful across various fields and industries. It supports several distance metrics, including Euclidean distance, cosine similarity, and inner-product distance, allowing you to tailor the search process to your needs. Faiss is adaptable for diverse applications like image similarity search, text document retrieval, and audio fingerprinting.

### Applications of FAISS

Faiss, the Facebook AI Similarity Search library, has been widely adopted across various industries due to its efficiency and scalability in handling large-scale vector data. Notable applications include:​

1. Trillion-Scale Indexing

Faiss has demonstrated the capability to index 1.5 trillion vectors, each with 144 dimensions. The process involves compressing vectors to 54 bytes using Principal Component Analysis (PCA) to reduce them to 72 dimensions, followed by a 6-bit scalar quantizer. A Hierarchical Navigable Small World (HNSW) graph with 10 million centroids serves as the coarse quantizer. The indexing process is distributed across multiple machines, culminating in a central server that manages search operations, achieving query response times of approximately one second. ​

2. Text Retrieval

In natural language processing, Faiss is utilized for knowledge-intensive tasks such as fact-checking, entity linking, slot filling, and open-domain question answering. By embedding large text corpora, Faiss enables efficient retrieval of relevant information, enhancing the performance and accuracy of language models. ​
arXiv

3. Data Mining

Faiss facilitates the mining and curation of extensive datasets. It has been employed to identify bilingual texts within vast web-crawled datasets and to organize language model training corpora by grouping documents with similar topics. In the image domain, Faiss assists in deduplicating large image datasets and curating subsets that match target distributions. ​
Continuum | Continuum Labs

4. Content Moderation

For content moderation, Faiss aids in detecting and managing harmful content at scale. Human-labeled examples of policy-violating images and videos are embedded using models like Self-Supervised Descriptor (SSCD) and stored in a Faiss index. New content is embedded and compared against this index to identify similarities, supporting a multi-stage classification pipeline that includes machine classification and human verification. ​

These applications underscore Faiss's versatility and effectiveness in handling large-scale similarity search and clustering tasks across diverse domains.

### Getting Started with FAIS and Langchain

In this section, we will show you how to set up Faiss and use it alongside LangChain and OpenAI embeddings.

#### Installation
To install Faiss, you can use pip to get the CPU or GPU version:

```
# For CPU
! pip install faiss-cpu

```

Install Langchain community, langchain-openai tiktoken and wikipedia

#### Component Integration for LLM Application Development

The construction of sophisticated Large Language Model (LLM) applications necessitates the integration of specialized components.

**LangChain:** Functions as an abstraction layer, facilitating the orchestration of LLM interactions with external data sources and execution environments. It provides a framework for developing complex, multi-step LLM workflows. The community portion extends this framework with a broad array of tools.

**langchain-openai:** Serves as a dedicated API wrapper, enabling seamless integration between LangChain and OpenAI's LLMs. It streamlines data transfer and command execution, optimizing communication.

**tiktoken:** A BPE tokenizer, crucial for accurate token estimation. It addresses LLM context window limitations and cost management by providing precise token counts for input and output text.

**Wikipedia:** We will use this library to download some articles using `WikipediaLoader`.

```python
from langchain.document_loaders import WikipediaLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

# Load content from Wikipedia using WikipediaLoader
loader = WikipediaLoader("Deep_learning")
documents = loader.load()
```


#### Chunking

Next we are going to chunk the documents loaded using langchain `ChunkTextSplitter`

```python
text_splitter = CharacterTextSplitter(chunk_size=1055, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
```

Next we will initiatize OpenAIEmbedding model

```python
import os
from google.colab import userdata
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
```


### Embedding Models


OpenAl offers two powerful third-generation embedding model (denoted by -3 in the model ID). Read the embedding v3 announcement blog post for more details.

| MODEL | -PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAXINPUT |
| :--- | :--- | :--- | :--- |
| text-embedding-3-small | 62,500 | $62.3 \%$ | 8191 |
| text-embedding-3-large | 9,615 | $64.6 \%$ | 8191 |
| text-embedding-ada-002 | 12,500 | $61.0 \%$ | 8191 |


```python
# Loading embeddings model
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```
Create FAISS based in memory database using docs and embeddings. We can also specify index as we will see later.


```python
# Convert documents to vectors and index vectors
db = FAISS.from_documents(docs, embeddings)
print(db.index.ntotal)
```

Total 90 docs in the index are created. Next we will create a search query and do a similarity search.


```python
# Search query
query = "What is Deep learning?"
docs = db.similarity_search(query)
print(docs[0].page_content)
```
Output is given below based on the page content retrieved.

    Deep learning is a form of machine learning that utilizes a neural network to transform a set of inputs into a set of outputs via an artificial neural network. Deep learning methods, often using supervised learning with labeled datasets, have been shown to solve tasks that involve handling complex, high-dimensional raw input data (such as images) with less manual feature engineering than prior methods, enabling significant progress in several fields including computer vision and natural language processing. In the past decade, deep RL has achieved remarkable results on a range of problems, from single and multiplayer games such as Go, Atari Games, and Dota 2 to robotics.
    
    
    === Reinforcement learning ===

### Summary

Faiss and LangChain are powerful tools often used together to build intelligent, scalable applications. Faiss, developed by Meta, is a high-performance library for efficient similarity search and clustering of dense vectors, making it ideal for handling large-scale data in tasks like recommendation systems and semantic search. LangChain, on the other hand, is a framework designed to connect large language models (LLMs) with external data sources and tools, enabling more dynamic and context-aware applications. When combined, LangChain can use Faiss as a vector store to enhance retrieval-augmented generation (RAG), allowing LLMs to access relevant information quickly and generate more accurate, informed responses.
Let us look at Faiss sample next without langchain.





`




