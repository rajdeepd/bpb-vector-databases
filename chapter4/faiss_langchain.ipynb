{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rajdeepd/bpb-vector-databases/blob/main/chapter4/faiss_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqCQPiPOI2_u"
   },
   "source": [
    "Reference: https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5H1Xt5MEXa_",
    "outputId": "6c125453-d91a-4319-bd8d-89eae9f9148f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "# For GPU\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wZkowIniEsU7"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koQJPHKxE8xl",
    "outputId": "46f9b600-7496-4443-ae8f-0e3d5719ea82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bTELVN17E1Nd"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7mBTDm35E3Ly"
   },
   "outputs": [],
   "source": [
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Deep_learning\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6tKXLAiFWLV",
    "outputId": "422d8cfc-9a2a-408f-f4ab-4ca36e8e5c99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1436, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1780, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1352, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1223, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1603, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1156, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1990, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2348, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1204, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1389, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1374, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1877, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1551, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1569, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1245, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1518, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1163, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2013, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1093, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2820, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1942, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1348, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2730, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2629, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1856, which is longer than the specified 1055\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1699, which is longer than the specified 1055\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1055, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IvzncmOwHs0z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qhRfjhODGfxQ"
   },
   "outputs": [],
   "source": [
    "# Loading embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McgUXfw0IDMl",
    "outputId": "a40a5508-3a7b-4020-900c-d8a9a3100ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Convert documents to vectors and index vectors\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXDx1UXQIIxD",
    "outputId": "9dfb2b6e-8d27-47d9-83bb-698544e95b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a form of machine learning that utilizes a neural network to transform a set of inputs into a set of outputs via an artificial neural network. Deep learning methods, often using supervised learning with labeled datasets, have been shown to solve tasks that involve handling complex, high-dimensional raw input data (such as images) with less manual feature engineering than prior methods, enabling significant progress in several fields including computer vision and natural language processing. In the past decade, deep RL has achieved remarkable results on a range of problems, from single and multiplayer games such as Go, Atari Games, and Dota 2 to robotics.\n",
      "\n",
      "\n",
      "=== Reinforcement learning ===\n"
     ]
    }
   ],
   "source": [
    "# Search query\n",
    "query = \"What is Deep learning?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+Gl0o4lsKrAlauHh6IUqy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
