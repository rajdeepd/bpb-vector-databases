{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rajdeepd/bpb-vector-databases/blob/main/chapter7/Evaluating_the_Ideal_Chunk_Size_for_a_RAG_System_using_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FqeieOC5vUB"
   },
   "source": [
    "# Evaluate the Ideal Chunk Size for a RAG System using LlamaIndex and GPT-4o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR8jlf3358_z"
   },
   "source": [
    "## **Setup**\n",
    "\n",
    "Before proceeding on the experiment, we need to ensure all required modules are imported. Make sure llama-index and pypdf python modules are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ItNWVKRRD67j"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y9SVm76h58de"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "import openai\n",
    "import time\n",
    "openai.api_key = 'sk-..'#'OPENAI-API-KEY' # set your openai api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvEzZzif6G5O"
   },
   "source": [
    "## **Download Data**\n",
    "\n",
    "We will be using the Uber 10K SEC Filings for 2021 for this experiment. First use `wget` to download the pdf into directory `data/10k/uber_2021.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZOD_9THEErrc",
    "outputId": "8d28932d-9470-4e66-e81d-f9cd23c9bd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-30 14:25:49--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1880483 (1.8M) [application/octet-stream]\n",
      "Saving to: ‘data/10k/uber_2021.pdf’\n",
      "\n",
      "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-09-30 14:25:50 (24.2 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO21UssT6L8N"
   },
   "source": [
    "## **Load Data**\n",
    "\n",
    "Let’s load our document using SimpleDirectoryReader.\n",
    "The SimpleDirectoryReader is the most commonly used data connector\n",
    "Pass in a input directory or a list of files.\n",
    "It selects the best file reader based on the file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x6QdEBd-17OC"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "reader = SimpleDirectoryReader(\"./data/10k/\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnpPtiz56TYA"
   },
   "source": [
    "## **Question Generation**\n",
    "\n",
    "To select the right `chunk_size`, we willcompute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help  generate questions from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "26BgDF3L6Z0r",
    "outputId": "be628589-f756-4eb1-f1df-6f80060060ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages of the document\n",
    "eval_documents = documents[:20]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3WwA-0N6dMO"
   },
   "source": [
    "## Setting Up Evaluators\n",
    "\n",
    "We are setting up the GPT-4o model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised.`Settings` is used for setting the llm, embed model, node parser etc .\n",
    "\n",
    "1. **Faithfulness Evaluator** - It helps measure if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
    "2. **Relevancy Evaluator** - It helps measure if the query was actually answered by the response and measures if the response + source nodes match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G2LoMRtr6fnG"
   },
   "outputs": [],
   "source": [
    "# We will be using GPT-4o for evaluating the responses\n",
    "gpt4o = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set model, embedding model chunk size etc using Settings\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# We will define Faithfulness and Relevancy Evaluators, based on GPT-4o\n",
    "\n",
    "faithfulness_gpt4o = FaithfulnessEvaluator()\n",
    "\n",
    "relevancy_gpt4o = RelevancyEvaluator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUncIIxR6gVz"
   },
   "source": [
    "## **Response Evaluation For A Chunk Size**\n",
    "\n",
    "We will evaluate each chunk_size based on 3 metrics.\n",
    "\n",
    "1. Average Response Time.\n",
    "2. Average Faithfulness.\n",
    "3. Average Relevancy.\n",
    "\n",
    "Function, `evaluate_response_time_and_accuracy`, that does  that which has:\n",
    "\n",
    "1. VectorIndex Creation.\n",
    "2. Building the Query Engine**.**\n",
    "3. Metrics Calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dEC2Lr0z6p1N"
   },
   "outputs": [],
   "source": [
    "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
    "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
    "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
    "\n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # create vector index\n",
    "    llm = OpenAI(model=\"gpt-4o\")\n",
    "    #service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
    "    #vector_index = VectorStoreIndex.from_documents(\n",
    "    #    eval_documents, service_context=service_context\n",
    "    #)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents\n",
    "    )\n",
    "    # bu\n",
    "    # build query engine\n",
    "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # Iterate over each question in eval_questions to compute metrics.\n",
    "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        faithfulness_result = faithfulness_gpt4o.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        relevancy_result = relevancy_gpt4o.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8DQvTP96s48"
   },
   "source": [
    "## **Testing Across Different Chunk Sizes**\n",
    "\n",
    "We will evaluate a range of chunk sizes to identify which offers the most promising metrics, then iterate over difference chunk size to evaluate metrics and get Average faithfullness and Average Relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jlKICwXH6Tib",
    "outputId": "10ae9d6c-b995-4058-afbd-5465dd5b5529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 128 - Average Response time: 2.79s, Average Faithfulness: 0.95, Average Relevancy: 0.93\n",
      "Chunk size 256 - Average Response time: 1.79s, Average Faithfulness: 0.95, Average Relevancy: 0.90\n",
      "Chunk size 512 - Average Response time: 1.98s, Average Faithfulness: 0.95, Average Relevancy: 0.93\n",
      "Chunk size 1024 - Average Response time: 2.07s, Average Faithfulness: 0.95, Average Relevancy: 0.97\n",
      "Chunk size 2048 - Average Response time: 2.03s, Average Faithfulness: 0.97, Average Relevancy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
    "\n",
    "for chunk_size in [128, 256, 512, 1024, 2048]:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size,eval_questions)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
