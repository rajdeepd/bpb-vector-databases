{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c69607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall --yes tensorflow==2.14\n",
    "#!pip install scipy\n",
    "#!pip install numpy\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917a482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce31e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699c7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df193128",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/mnist.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0fa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gzip\n",
    "import _pickle as cPickle\n",
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "if sys.version_info < (3,):\n",
    "    data = cPickle.load(f)\n",
    "else:\n",
    "    data = cPickle.load(f, encoding='bytes')\n",
    "f.close()\n",
    "(x_tr, y_tr), (x_te, y_te) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34484605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e51b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te = x_tr.astype('float32')/255., x_te.astype('float32')/255.\n",
    "x_tr_flat, x_te_flat = x_tr.reshape(x_tr.shape[0], -1), x_te.reshape(x_te.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5a586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape, x_te.shape)\n",
    "print(x_tr_flat.shape, x_te_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4eadb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Parameters\n",
    "batch_size, n_epoch = 100, 50\n",
    "n_hidden, z_dim = 256, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd25a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a training image\n",
    "plt.figure(figsize = (2,2))\n",
    "plt.imshow(x_tr[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2d5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling function\n",
    "def sampling(args):\n",
    "    mu, log_var = args\n",
    "    eps = K.random_normal(shape=(batch_size, z_dim), mean=0., stddev=1.0)\n",
    "    return mu + K.exp(log_var) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfce400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder - from 784->256->128->2\n",
    "inputs_flat = Input(shape=(x_tr_flat.shape[1:]))\n",
    "x_flat = Dense(n_hidden, activation='relu')(inputs_flat) # first hidden layer\n",
    "x_flat = Dense(n_hidden//2, activation='relu')(x_flat)  # second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db223571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden state, which we will pass into the Model to get the Encoder.\n",
    "mu_flat = Dense(z_dim)(x_flat)\n",
    "log_var_flat = Dense(z_dim)(x_flat)\n",
    "z_flat = Lambda(sampling, output_shape=(z_dim,))([mu_flat, log_var_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1127ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized type for `inputs`: KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\") (of type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m z_decoded \u001b[38;5;241m=\u001b[39m z_decoder2(z_decoded)\n\u001b[1;32m      8\u001b[0m y_decoded \u001b[38;5;241m=\u001b[39m y_decoder(z_decoded)\n\u001b[0;32m----> 9\u001b[0m decoder_flat \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_decoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder_conv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m outputs_flat \u001b[38;5;241m=\u001b[39m decoder_flat(z_flat)\n",
      "File \u001b[0;32m~/work/github/rajdeepd/bpb-vector-databases/my-python3-env/lib/python3.10/site-packages/keras/src/models/model.py:143\u001b[0m, in \u001b[0;36mModel.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m functional_init_arguments(args, kwargs) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m==\u001b[39m Model:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunctional\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/work/github/rajdeepd/bpb-vector-databases/my-python3-env/lib/python3.10/site-packages/keras/src/utils/tracking.py:28\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/github/rajdeepd/bpb-vector-databases/my-python3-env/lib/python3.10/site-packages/keras/src/models/functional.py:124\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen providing `inputs` as a list/tuple, all values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the list/tuple must be KerasTensors. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, backend\u001b[38;5;241m.\u001b[39mKerasTensor):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized type for `inputs`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized type for `inputs`: KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\") (of type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>)"
     ]
    }
   ],
   "source": [
    "#Decoder - from 2->128->256->784\n",
    "latent_inputs = Input(shape=(z_dim,))\n",
    "z_decoder1 = Dense(n_hidden//2, activation='relu')\n",
    "z_decoder2 = Dense(n_hidden, activation='relu')\n",
    "y_decoder = Dense(x_tr_flat.shape[1], activation='sigmoid')\n",
    "z_decoded = z_decoder1(latent_inputs)\n",
    "z_decoded = z_decoder2(z_decoded)\n",
    "y_decoded = y_decoder(z_decoded)\n",
    "decoder_flat = Model(latent_inputs, y_decoded, name=\"decoder_conv\")\n",
    "\n",
    "outputs_flat = decoder_flat(z_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a8904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational autoencoder (VAE) - to reconstruction input\n",
    "reconstruction_loss = losses.binary_crossentropy(inputs_flat,\n",
    "                                                 outputs_flat) * x_tr_flat.shape[1]\n",
    "kl_loss = 0.5 * K.sum(K.square(mu_flat) + K.exp(log_var_flat) - log_var_flat - 1, axis = -1)\n",
    "vae_flat_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "# Build model\n",
    "#  Ensure that the reconstructed outputs are as close to the inputs\n",
    "vae_flat = Model(inputs_flat, outputs_flat)\n",
    "vae_flat.add_loss(vae_flat_loss)\n",
    "vae_flat.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f98ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
