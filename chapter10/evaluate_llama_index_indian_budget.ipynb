{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rajdeepd/bpb-vector-databases/blob/main/chapter10/evaluate_llama_index_indian_budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D8OtrFwl05l",
    "outputId": "1de5e75c-65b8-4879-914e-b4fb95cd1d94"
   },
   "outputs": [],
   "source": [
    "#!pip install -q llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install -q llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U7Evow7CmDkj"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxlcJPaZb_m7",
    "outputId": "04f4f80c-be2b-4038-c9bd-700bcecfd5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = ''\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
    "    len(OPENAI_API_KEY)\n",
    "else:\n",
    "    from jproperties import Properties\n",
    "\n",
    "    configs = Properties()\n",
    "    with open('../config.properties', 'rb') as config_file:\n",
    "        configs.load(config_file)\n",
    "        print(configs.get(\"OPENAI_API_KEY\").data[:5]) \n",
    "        OPENAI_API_KEY = configs.get(\"OPENAI_API_KEY\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q7tBgeKm6mS"
   },
   "source": [
    "# Setup the OpenAI API key\n",
    "We will setup the OPENAI_API_KEY in the environment variable. This will be used by LlamaIndex as we have seen earier. We are also checking validity of the key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "dIDjKGj8mGNj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "#OPENAI_API_KEY\n",
    "len(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a valid OpenAI API key.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "def check_openai_api_key(api_key):\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    try:\n",
    "    t except openai.AuthenticationError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "if check_openai_api_key(OPENAI_API_KEY):\n",
    "    print(\"It is a valid OpenAI API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzcgZggwnLI4"
   },
   "source": [
    "# **Indian Budget Speech from 2025.**\n",
    "\n",
    "**Load Data**\n",
    "We will check if the notebook is running in Google colab or not with a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CPGqcf0kkdR1"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    \n",
    "    # This will prompt for authorization.\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the flag load appropriate path into `PATH` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74qLcEaYkmuC",
    "outputId": "fd437475-ffb2-44bd-b2bc-1153ab25504d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian_budget_speech_2025.md\n"
     ]
    }
   ],
   "source": [
    "PATH= ''\n",
    "if IN_COLAB:\n",
    "    #!ls \"/content/indian_budget\"\n",
    "    PATH=\"/content/indian_budget\"\n",
    "else:\n",
    "    PATH='../data/indian_budget'\n",
    "# Check that PATH is showing the markdown we are interested in\n",
    "!ls $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF2pRwM0nZXZ"
   },
   "source": [
    "# **Load Data and Build Index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_d9-jp0pnbnZ"
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(PATH).load_data()\n",
    "\n",
    "# Define an LLM we want to use, gpt-4 in thsi case.\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Build the vectorindex with a chunk_size of 512\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUVqDhzOlzg8",
    "outputId": "0a2c873d-4acb-41cc-e648-bcb8418eee8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 29 nodes loaded from the document.As we learnt earlier each node is a chunk from previous document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2a8_KfZngo0"
   },
   "source": [
    "# **Build a QueryEngine and Query.**\n",
    "We will build query engine which can take plain text queries and try to find closest responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "oRLPNGqUnj_M"
   },
   "outputs": [],
   "source": [
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "# Query Engine by default retrieves two similar nodes/ chunks. It can be modify using following APIs\n",
    "# vector_index.as_query_engine(similarity_top_k=k).\n",
    "\n",
    "\n",
    "response_vector_1 = query_engine.query(\"What is the  revised estimate of total expenditure?\")\n",
    "response_vector_2 = query_engine.query(\"What is total fiscal deficit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlQViVddnlGJ",
    "outputId": "7d2070e3-75a6-4325-ab35-fd8b1b3aef41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_vector_1.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-nyz0-dnrfR"
   },
   "source": [
    "**print and check the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0NWSvvE6nvkM",
    "outputId": "3799eab4-ff3f-447f-c16b-325f4f104a42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The revised estimate of total expenditure is ₹47.16 lakh crore.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_vector_1.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ck3dmb3Dy38R",
    "outputId": "68fd225a-8c87-4c07-d603-08274015e3aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total fiscal deficit is estimated to be 4.4 per cent of GDP.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_vector_2.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzG68cHKoQR4",
    "outputId": "fd150a14-924b-4757-9639-5103d8214fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of chunks retrieved.\n",
    "len(response_vector.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDzjvo89pwZA"
   },
   "source": [
    "# Build Evaluation Data Set\n",
    "## Question-Context Pair Generation\n",
    "Let us now build a simple evaluation dataset over the existing text corpus we created above\n",
    "\n",
    "We use method generate_question_context_pairs to generate a set of (question, context) pairs over the unstructured text corpus. This method uses the LLM to auto-generate questions from each context chunk.\n",
    "\n",
    "the method returns  a EmbeddingQAFinetuneDataset object. Tthis contains at a high level  a set of ids mapping to queries and relevant chunks and the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQg5IemqpuCi",
    "outputId": "e8efc4b9-bcc3-4cba-f614-084a50379a5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 29/29 [01:33<00:00,  3.22s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built the dataset for evaluation we will not create a retriever and evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAJ6e1lkp2DL"
   },
   "source": [
    "# **Retrieval Evaluation:**\n",
    "Retrievers help fetch the most relevant context based on the user query. We will build it based on the index created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RagG50SSp4LA"
   },
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSHbUMLZp9ZZ"
   },
   "source": [
    "# **Mean Reciprocal Rank (MRR):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CzS6LjO9p8ob"
   },
   "outputs": [],
   "source": [
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BOOGKWduqCsb"
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y9TTNfOGqGlK"
   },
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "ymOL8TD_qLBc",
    "outputId": "41ac3090-8bc6-4545-dedd-50f4237471f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI Embedding Retriever</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.775862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Retriever Name  Hit Rate       MRR\n",
       "0  OpenAI Embedding Retriever  0.931034  0.775862"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"OpenAI Embedding Retriever\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3ttytVVw_WU"
   },
   "source": [
    "# **Response Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YTY0CUNQxEzc"
   },
   "outputs": [],
   "source": [
    "# Get the list of queries from the above created dataset\n",
    "\n",
    "queries = list(qa_dataset.queries.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhZ50rkGxILf",
    "outputId": "b503e5a3-f050-4042-833b-47b7bdf56868"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In the Budget 2025-2026 speech by Nirmala Sitharaman, what are the five key objectives that the Government's efforts are focused on, as mentioned in the introduction?\",\n",
       " \"According to the table of contents, what are the four 'engines' and the 'fuel' that are discussed in Part A of the budget speech?\",\n",
       " \"What are the five key objectives of the Government's budget as mentioned in the introduction of the document?\",\n",
       " \"According to the document, what are the six aspects that encompass 'Viksit Bharat' as per the great Telugu poet and playwright Gurajada Appa Rao's perspective?\",\n",
       " '\"What are the six domains that the Budget aims to initiate transformative reforms in over the next five years, according to the context provided?\"']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upjvFWiPxLVX"
   },
   "source": [
    "# **Faithfulness Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D6aa4UhsxKc1"
   },
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "#gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "#Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "#Settings.num_output = 512\n",
    "#Settings.context_window = 3900\n",
    "##service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
    "\n",
    "# gpt-3.5-turbo-16k-0613\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "#service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "#gpt-4\n",
    "#gpt35T = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "#service_context_gpt4 = ServiceContext.from_defaults(llm=gpt35T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NbUYFuE2xxyc"
   },
   "outputs": [],
   "source": [
    "#Create a QueryEngine with gpt-3.5-turbo service_context to generate response for the query.\n",
    "#vector_index = VectorStoreIndex(nodes, service_context = service_context_gpt35)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "#query_engine = VectorStoreIndex.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Yo0ziJl0x2go"
   },
   "outputs": [],
   "source": [
    "#Create a FaithfulnessEvaluator\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "#faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "faithfulness_gpt4  = FaithfulnessEvaluator(llm=gpt4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "x4zuyDS6x8_3",
    "outputId": "2348143d-901c-45f4-b254-7830dc4e069a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the table of contents, what are the four 'engines' and the 'fuel' that are discussed in Part A of the budget speech?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_query = queries[1]\n",
    "\n",
    "eval_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ept1j-uIx-M-"
   },
   "outputs": [],
   "source": [
    "#Generate response first and use faithfull evaluator.\n",
    "response_vector = query_engine.query(eval_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qCb11ccuyDsk"
   },
   "outputs": [],
   "source": [
    "# Compute faithfulness evaluation\n",
    "\n",
    "eval_result = faithfulness_gpt4.evaluate_response(response=response_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIHWBLiryDmb",
    "outputId": "b5a556de-b7c3-4e0c-dd48-a2a853a92d73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check passing parameter in eval_result if it passed the evaluation.\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioIvPMM7yHOF"
   },
   "source": [
    "# **Relevancy Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vhZyZd-AyLUz"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "#relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "relevancy_gpt4 = RelevancyEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gIX3WHxvySkC",
    "outputId": "df876da1-c2ec-488f-85d2-4b4a803b5026"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the table of contents, what are the four 'engines' and the 'fuel' that are discussed in Part A of the budget speech?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a query\n",
    "query = queries[1]\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Tk1WJ88fyVJV"
   },
   "outputs": [],
   "source": [
    "# Generate response.\n",
    "# response_vector has response and source nodes (retrieved context)\n",
    "response_vector = query_engine.query(query)\n",
    "\n",
    "# Relevancy evaluation\n",
    "eval_result = relevancy_gpt4.evaluate_response(\n",
    "    query=query, response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zn0GhJKgyW_N",
    "outputId": "d1eaef4c-9445-4bb3-94ea-443316af86d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check passing parameter in eval_result if it passed the evaluation.\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eqySQ7iVyZSw",
    "outputId": "e8b592fd-8e42-4304-985a-0a2e9ebb54a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YES'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get the feedback for the evaluation.\n",
    "eval_result.feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HqXLL8_ydBx"
   },
   "source": [
    "# **Batch Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "CouVNEgDycsM"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "# Let's pick top 10 queries to do evaluation\n",
    "batch_eval_queries = queries[:10]\n",
    "\n",
    "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
    "runner = BatchEvalRunner(\n",
    "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
    "    workers=8,\n",
    ")\n",
    "\n",
    "# Compute evaluation\n",
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine, queries=batch_eval_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZR0KgheykrX",
    "outputId": "f74e6713-b964-47ac-d940-b0fd1564f791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get faithfulness score\n",
    "\n",
    "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsBtSM-Vym1B",
    "outputId": "4cc376aa-4190-45d9-9fe1-90fb0e49f294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get relevancy score\n",
    "\n",
    "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "relevancy_score\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "envjan2025",
   "language": "python",
   "name": "envjan2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
