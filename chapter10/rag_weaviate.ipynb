{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMWnkIkkPh3W"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/rag_weaviate.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9kcX2B_atc"
      },
      "source": [
        "# RAG with Weaviate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0407qEfUPh3Z"
      },
      "source": [
        "\n",
        "This is a code sample that uses [Weaviate](https://weaviate.io/) to perform RAG over PDF documents parsed by [Docling](https://ds4sd.github.io/docling/). Table below outlines the LLM model used and whether it is local or remote component being used\n",
        "\n",
        "| Step | LLM Model | Execution  - Local or Remote|\n",
        "| --- | --- | --- |\n",
        "| Embedding | Open AI | ðŸŒ Remote |\n",
        "| Vector store | Weavieate |ðŸŒ Remote |\n",
        "| Gen AI | Open AI | ðŸŒ Remote |\n",
        "\n",
        "In this example, we accomplish the following tasks:\n",
        "* Parse the top machine learning papers on [arXiv](https://arxiv.org/) using Docling\n",
        "* Perform hierarchical chunking of the documents using Docling\n",
        "* Generate text embeddings with OpenAI\n",
        "* Perform RAG using [Weaviate](https://weaviate.io/developers/weaviate/search/generative)\n",
        "\n",
        "In this example, we will accomplish the following tasks:\n",
        "* We will parse the top machine learning papers on arXiv using Docling. After that we will [erform hierarchical chunking of the documents using Docling HierarchicalChunker\n",
        "Then Generate text embeddings with OpenAI\n",
        "* In the end we will perform RAG using Weaviate vector store\n",
        "To run this sample, you'll need:the following\n",
        "\n",
        "To run this notebook, you'll need:\n",
        "* An [OpenAI API key](https://platform.openai.com/docs/quickstart)\n",
        "* Access to GPU/s\n",
        "\n",
        "Note: For best results, please use **GPU acceleration** to run this notebook. Here are two options for running this notebook:\n",
        "1. **Locally on a MacBook with an Apple Silicon chip.** Docling's has a MPS accelerators for macbook\n",
        "2. **Run this code on Google Colab.** Convert all documents in the notebook takes about8 mintutes on a Google Colab T4 GPU.\n",
        "\n",
        "\n",
        "Run this notebook on Google Colab. Converting all documents in the notebook takes approximately~8 mintutes on a Google Colab T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YgT7tpXCUl0"
      },
      "source": [
        "### Install Docling and Weaviate client\n",
        "\n",
        "Note: If Colab prompts you to restart the session after running the cell below, click \"restart\" and proceed with running the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "u076oUSF_YUG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install docling~=\"2.25.2\"\n",
        "%pip install -U weaviate-client~=\"4.11.1\"\n",
        "%pip install rich\n",
        "%pip install torch\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "\n",
        "# we will change the log level for Weaviate client\n",
        "logging.getLogger(\"weaviate\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q2F9RUmR8Wj"
      },
      "source": [
        "## Docling\n",
        "\n",
        "Docling can run on commodity hardware. In our case we ran it on Google colab where Tesla T4 was the GPU with cuda enabled. On local Macbook it integrates with  b\n",
        "Part of what makes Docling so remarkable is the fact that it can run on commodity hardware. This means that this notebook can be run on a local machine with GPU acceleration. If you're using a MacBook with a silicon chip, Docling integrates seamlessly with Metal Performance Shaders (MPS). MPS provides out-of-the-box GPU acceleration for macOS, seamlessly integrating with PyTorch and TensorFlow, offering energy-efficient performance on Apple Silicon, and broad compatibility with all Metal-supported GPUs.\n",
        "\n",
        "The code below is provided to checks to see if a GPU is available, either of CUDA or MPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o18TT5vZPh3b",
        "outputId": "d10845ee-dfb4-4040-e77c-1977a11dfb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA GPU is enabled: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU or MPS is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"CUDA GPU is enabled: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"MPS GPU is enabled.\")\n",
        "else:\n",
        "    raise EnvironmentError(\n",
        "        \"No GPU or MPS device found. Please check your environment and ensure GPU or MPS support is configured.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTsy4a8JFPl"
      },
      "source": [
        "Here, we've collected 10 influential machine learning papers published as PDFs on arXiv. Because Docling does not yet have title extraction for PDFs, we manually add the titles in a corresponding list.\n",
        "\n",
        "Note: Converting all 10 papers should take around 8 minutes with a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vy5SMPiGDMy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df30da7-0251-4176-9d5b-c0d3dc1017fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['http://arxiv.org/abs/2303.08774v3', 'http://arxiv.org/abs/2307.09288v2', 'http://arxiv.org/abs/2302.13971v1', 'http://arxiv.org/abs/2303.12712v5', 'http://arxiv.org/abs/2306.05685v3', 'http://arxiv.org/abs/2301.12597v3', 'http://arxiv.org/abs/2304.02643v1', 'http://arxiv.org/abs/2305.10403v3', 'http://arxiv.org/abs/2306.01116v1', 'http://arxiv.org/abs/2303.03378v1']\n"
          ]
        }
      ],
      "source": [
        "# Influential machine learning papers\n",
        "arxiv_urls = [\n",
        "    \"http://arxiv.org/abs/2303.08774v3\",\n",
        "    \"http://arxiv.org/abs/2307.09288v2\",\n",
        "    \"http://arxiv.org/abs/2302.13971v1\",\n",
        "    \"http://arxiv.org/abs/2303.12712v5\",\n",
        "    \"http://arxiv.org/abs/2306.05685v3\",\n",
        "    \"http://arxiv.org/abs/2301.12597v3\",\n",
        "    \"http://arxiv.org/abs/2304.02643v1\",\n",
        "    \"http://arxiv.org/abs/2305.10403v3\",\n",
        "    \"http://arxiv.org/abs/2306.01116v1\",\n",
        "    \"http://arxiv.org/abs/2303.03378v1\",\n",
        "]\n",
        "\n",
        "print(arxiv_urls)\n",
        "\n",
        "source_titles =[\n",
        "    \"GPT-4 Technical Report\",\n",
        "    \"Llama 2: Open Foundation and Fine-Tuned Chat Models\",\n",
        "    \"LLaMA: Open and Efficient Foundation Language Models\",\n",
        "    \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\",\n",
        "    \"Judging LLM-as-a-Judge with MTBench and Chatbot Arena\",\n",
        "    \"BLIP-2: Bootstrapping LanguageImage Pre-training with Frozen Image Encoders and Large Language Models\",\n",
        "    \"Segment Anything\",\n",
        "    \"PaLM 2 Technical Report\",\n",
        "    \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\",\n",
        "    \"PaLM-E: An Embodied Multimodal Language Model\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And their corresponding titles (because Docling doesn't have title extraction yet!)\n",
        "```\n",
        "source_titles = [\n",
        "    \"Attention Is All You Need\",\n",
        "    \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
        "    \"Generative Adversarial Nets\",\n",
        "    \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n",
        "    \"Adam: A Method for Stochastic Optimization\",\n",
        "    \"Auto-Encoding Variational Bayes\",\n",
        "    \"Playing Atari with Deep Reinforcement Learning\",\n",
        "    \"Deep Residual Learning for Image Recognition\",\n",
        "    \"Sequence to Sequence Learning with Neural Networks\",\n",
        "    \"A Neural Probabilistic Language Model\",\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "IeqShfDU_zFx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fi8wzHrCoLa"
      },
      "source": [
        "### Convert PDFs to Docling documents\n",
        "\n",
        "Here we use Docling's `.convert_all()` to parse a batch of PDFs. The result is a list of Docling documents that we can use for text extraction.\n",
        "\n",
        "Note: Please ignore the `ERR#` message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sr44xGR1PNSc"
      },
      "outputs": [],
      "source": [
        "from docling.datamodel.document import ConversionResult\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# Instantiate the doc converter\n",
        "doc_converter = DocumentConverter()\n",
        "\n",
        "# Directly pass list of files or streams to `convert_all`\n",
        "conv_results_iter = doc_converter.convert_all(arxiv_urls)  # previously `convert`\n",
        "\n",
        "# Iterate over the generator to get a list of Docling documents\n",
        "docs = [result.document for result in conv_results_iter]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHun_P-OCtKd"
      },
      "source": [
        "### Post-process extracted document data\n",
        "#### Perform hierarchical chunking on documents\n",
        "\n",
        "We use Docling's `HierarchicalChunker()` to perform hierarchy-aware chunking of our list of documents. This is meant to preserve some of the structure and relationships within the document, which enables more accurate and relevant retrieval in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L17ju9xibuIo"
      },
      "outputs": [],
      "source": [
        "from docling_core.transforms.chunker import HierarchicalChunker\n",
        "\n",
        "# Initialize lists for text, and titles\n",
        "texts, titles = [], []\n",
        "\n",
        "chunker = HierarchicalChunker()\n",
        "\n",
        "# Process each document in the list\n",
        "for doc, title in zip(docs, source_titles):  # Pair each document with its title\n",
        "    chunks = list(\n",
        "        chunker.chunk(doc)\n",
        "    )  # Perform hierarchical chunking and get text from chunks\n",
        "    for chunk in chunks:\n",
        "        texts.append(chunk.text)\n",
        "        titles.append(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khbU9R1li2Kj"
      },
      "source": [
        "Because we're splitting the documents into chunks, we'll concatenate the article title to the beginning of each chunk for additional context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HNwYV9P57OwF"
      },
      "outputs": [],
      "source": [
        "# Concatenate title and text\n",
        "for i in range(len(texts)):\n",
        "    texts[i] = f\"{titles[i]} {texts[i]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhLlCpQODaT3"
      },
      "source": [
        "## ðŸ’š Part 2: Weaviate\n",
        "### Create and configure an embedded Weaviate collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho7xYQTZK5Wk"
      },
      "source": [
        "We'll be using the OpenAI API for both generating the text embeddings and for the generative model in our RAG pipeline. The code below dynamically fetches your API key based on whether you're running this notebook in Google Colab and running it as a regular Jupyter notebook. All you need to do is replace `openai_api_key_var` with the name of your environmental variable name or Colab secret name for the API key.\n",
        "\n",
        "If you're running this notebook in Google Colab, make sure you [add](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) your API key as a secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PD53jOT4roj2"
      },
      "outputs": [],
      "source": [
        "# OpenAI API key variable name\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"  # Replace with the name of your secret/env var\n",
        "\n",
        "# Fetch OpenAI API key\n",
        "try:\n",
        "    # If running in Colab, fetch API key from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API key from environment variable\n",
        "    import os\n",
        "\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "WEAVIATE_URL=userdata.get('WEAVIATE_URL')\n",
        "WEAVIATE_API_KEY = userdata.get('WEAVIATE_API_KEY')"
      ],
      "metadata": {
        "id": "lEiaVnCpAV4T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "from weaviate.classes.init import AdditionalConfig, Timeout\n",
        "# Best practice: store your credentials in environment variables\n",
        "weaviate_url = WEAVIATE_URL #os.environ[\"WEAVIATE_URL\"]\n",
        "weaviate_api_key = WEAVIATE_API_KEY\n",
        "\n",
        "# Connect to Weaviate Cloud\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=weaviate_url,\n",
        "    auth_credentials=Auth.api_key(weaviate_api_key),headers={\"X-OpenAI-Api-Key\": openai_api_key},\n",
        "    additional_config=AdditionalConfig(\n",
        "        timeout=Timeout(init=30, query=60, insert=120)  # Values in seconds\n",
        "    )\n",
        ")\n",
        "\n",
        "print(client.is_ready())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GgskJJ-X5GP",
        "outputId": "d5e2a809-3c36-44a5-babe-7c628df8a855"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5jZSh6ti3e"
      },
      "source": [
        "[Embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) allows you to spin up a Weaviate instance directly from your application code, without having to use a Docker container. If you're interested in other deployment methods, like using Docker-Compose or Kubernetes, check out this [page](https://weaviate.io/developers/weaviate/installation) in the Weaviate docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hFUBEZiJUMic"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "\n",
        "# Connect to Weaviate embedded\n",
        "#client = weaviate.connect_to_embedded(headers={\"X-OpenAI-Api-Key\": openai_api_key})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4nu9qM75hrsd"
      },
      "outputs": [],
      "source": [
        "import weaviate.classes.config as wc\n",
        "from weaviate.classes.config import DataType, Property\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"docling_latest_papers\"\n",
        "\n",
        "# Delete the collection if it already exists\n",
        "if client.collections.exists(collection_name):\n",
        "    client.collections.delete(collection_name)\n",
        "\n",
        "# Create the collection\n",
        "collection = client.collections.create(\n",
        "    name=collection_name,\n",
        "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(\n",
        "        model=\"text-embedding-3-large\",  # Specify your embedding model here\n",
        "    ),\n",
        "    # Enable generative model from Cohere\n",
        "    generative_config=wc.Configure.Generative.openai(\n",
        "        model=\"gpt-4o\"  # Specify your generative model for RAG here\n",
        "    ),\n",
        "    # Define properties of metadata\n",
        "    properties=[\n",
        "        wc.Property(name=\"text\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"title\", data_type=wc.DataType.TEXT, skip_vectorization=True),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgMcZDB9Dzfs"
      },
      "source": [
        "### Wrangle data into an acceptable format for Weaviate\n",
        "\n",
        "Transform our data from lists to a list of dictionaries for insertion into our Weaviate collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kttDgwZEsIJQ"
      },
      "outputs": [],
      "source": [
        "# Initialize the data object\n",
        "data = []\n",
        "\n",
        "# Create a dictionary for each row by iterating through the corresponding lists\n",
        "for text, title in zip(texts, titles):\n",
        "    data_point = {\n",
        "        \"text\": text,\n",
        "        \"title\": title,\n",
        "    }\n",
        "    data.append(data_point)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_rows = [\n",
        "    {\"title\": f\"Object {i+1}\"} for i in range(5)\n",
        "]\n",
        "\n",
        "#collection = client.collections.get(\"MyCollection\")\n",
        "\n",
        "with collection.batch.dynamic() as batch:\n",
        "    for data_row in data:\n",
        "        batch.add_object(\n",
        "            properties=data_row,\n",
        "        )\n",
        "        if batch.number_errors > 10:\n",
        "            print(\"Batch import stopped due to excessive errors.\")\n",
        "            break\n",
        "\n",
        "failed_objects = collection.batch.failed_objects\n",
        "if failed_objects:\n",
        "    print(f\"Number of failed imports: {len(failed_objects)}\")\n",
        "    print(f\"First failed object: {failed_objects[0]}\")"
      ],
      "metadata": {
        "id": "cKwFJZ-oBHMH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4amqRaoD5g0"
      },
      "source": [
        "### Insert data into Weaviate and generate embeddings\n",
        "\n",
        "Embeddings will be generated upon insertion to our Weaviate collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g8VCYnhbaxcz"
      },
      "outputs": [],
      "source": [
        "# Insert text chunks and metadata into vector DB collection\n",
        "# response = collection.data.insert_many(data)\n",
        "\n",
        "#if response.has_errors:\n",
        "#    print(response.errors)\n",
        "#else:\n",
        "#.   print(\"Insert complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmgSoYBOmfNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI01PxjuD_XR"
      },
      "source": [
        "### Query the data\n",
        "\n",
        "Here, we perform a simple similarity search to return the most similar embedded chunks to our search query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbz6nWJc5CSj",
        "outputId": "90862549-f172-4c3e-af4a-f2fb57dc5ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'GPT-4 Technical Report ()', 'title': 'GPT-4 Technical Report'}\n",
            "0.39860624074935913\n",
            "{'text': 'GPT-4 Technical Report (cs)', 'title': 'GPT-4 Technical Report'}\n",
            "0.4060209393501282\n"
          ]
        }
      ],
      "source": [
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "response = collection.query.near_text(\n",
        "    query=\"GPT-4\",\n",
        "    limit=2,\n",
        "    return_metadata=MetadataQuery(distance=True),\n",
        "    return_properties=[\"text\", \"title\"],\n",
        ")\n",
        "\n",
        "for o in response.objects:\n",
        "    print(o.properties)\n",
        "    print(o.metadata.distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elo32iMnEC18"
      },
      "source": [
        "### Perform RAG on parsed articles\n",
        "\n",
        "Weaviate's `generate` module allows you to perform RAG over your embedded data without having to use a separate framework.\n",
        "\n",
        "We specify a prompt that includes the field we want to search through in the database (in this case it's `text`), a query that includes our search term, and the number of retrieved results to use in the generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "7r2LMSX9bO4y",
        "outputId": "d4ecc5ae-7d1a-4dba-f00f-76f1c7b53749"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mâ•­â”€\u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31mâ”€â•®\u001b[0m\n",
              "\u001b[1;31mâ”‚\u001b[0m Explain how GPT-4 is doing on various benchmarks, using only the retrieved context.                             \u001b[1;31mâ”‚\u001b[0m\n",
              "\u001b[1;31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span> Explain how GPT-4 is doing on various benchmarks, using only the retrieved context.                             <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mâ•­â”€\u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32mâ”€â•®\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m To explain how GPT-4 is performing on various benchmarks, we would need to refer to the specific details        \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m provided in the GPT-4 Technical Report. However, since the context provided does not include specific benchmark \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m results or details, I can offer a general overview based on typical content found in such reports.              \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m In general, a technical report for a model like GPT-4 would include performance evaluations across a range of   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m benchmarks, which might include:                                                                                \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 1. **Natural Language Understanding (NLU) Benchmarks**: These could involve tasks like reading comprehension,   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m sentiment analysis, and question answering. GPT-4 would be evaluated on datasets such as SQuAD, GLUE, or        \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m SuperGLUE, where it would be expected to demonstrate improvements over previous models in understanding and     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m generating human-like text.                                                                                     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 2. **Language Generation Benchmarks**: This would assess the model's ability to generate coherent and           \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m contextually relevant text. Benchmarks might include tasks like story completion or dialogue generation, where  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m GPT-4's outputs would be compared to human-written text for fluency and relevance.                              \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 3. **Multimodal Benchmarks**: If applicable, GPT-4 might be evaluated on its ability to handle inputs beyond    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m text, such as images or audio, and perform tasks that require understanding and generating content across       \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m different modalities.                                                                                           \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 4. **Domain-Specific Benchmarks**: These could involve specialized tasks in areas like legal, medical, or       \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m technical domains, where the model's ability to understand and generate domain-specific language is tested.     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 5. **Ethical and Bias Benchmarks**: The report might also include evaluations of GPT-4's performance in terms   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m of ethical considerations, such as reducing bias and ensuring fairness in its outputs.                          \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m For precise details on how GPT-4 performs on these or other specific benchmarks, one would need to refer        \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m directly to the sections of the GPT-4 Technical Report that discuss these evaluations.                          \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generated Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> To explain how GPT-4 is performing on various benchmarks, we would need to refer to the specific details        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> provided in the GPT-4 Technical Report. However, since the context provided does not include specific benchmark <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> results or details, I can offer a general overview based on typical content found in such reports.              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> In general, a technical report for a model like GPT-4 would include performance evaluations across a range of   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> benchmarks, which might include:                                                                                <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 1. **Natural Language Understanding (NLU) Benchmarks**: These could involve tasks like reading comprehension,   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> sentiment analysis, and question answering. GPT-4 would be evaluated on datasets such as SQuAD, GLUE, or        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> SuperGLUE, where it would be expected to demonstrate improvements over previous models in understanding and     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> generating human-like text.                                                                                     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 2. **Language Generation Benchmarks**: This would assess the model's ability to generate coherent and           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> contextually relevant text. Benchmarks might include tasks like story completion or dialogue generation, where  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> GPT-4's outputs would be compared to human-written text for fluency and relevance.                              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 3. **Multimodal Benchmarks**: If applicable, GPT-4 might be evaluated on its ability to handle inputs beyond    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> text, such as images or audio, and perform tasks that require understanding and generating content across       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> different modalities.                                                                                           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 4. **Domain-Specific Benchmarks**: These could involve specialized tasks in areas like legal, medical, or       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> technical domains, where the model's ability to understand and generate domain-specific language is tested.     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 5. **Ethical and Bias Benchmarks**: The report might also include evaluations of GPT-4's performance in terms   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> of ethical considerations, such as reducing bias and ensuring fairness in its outputs.                          <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> For precise details on how GPT-4 performs on these or other specific benchmarks, one would need to refer        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> directly to the sections of the GPT-4 Technical Report that discuss these evaluations.                          <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "\n",
        "# Create a prompt where context from the Weaviate collection will be injected\n",
        "prompt = \"Explain how {text} is doing on various benchmarks, using only the retrieved context.\"\n",
        "query = \"GPT-4\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query, limit=3, grouped_task=prompt, return_properties=[\"text\", \"title\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "\n",
        "console.print(\n",
        "    Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\")\n",
        ")\n",
        "console.print(\n",
        "    Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "Dtju3oCiDOdD",
        "outputId": "3def7674-2680-4b4c-b3b2-b17eb667d6ad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mâ•­â”€\u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31mâ”€â•®\u001b[0m\n",
              "\u001b[1;31mâ”‚\u001b[0m Explain how LLAMA has been trained.                                                                             \u001b[1;31mâ”‚\u001b[0m\n",
              "\u001b[1;31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span> Explain how LLAMA has been trained.                                                                             <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mâ•­â”€\u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32mâ”€â•®\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m The text provided appears to be a collection of snippets related to \"Llama 2: Open Foundation and Fine-Tuned    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m Chat Models\" and its association with arXivLabs. However, the text does not provide specific details on how     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m Llama 2 has been trained. To explain how Llama 2 has been trained, we would typically consider the following    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m aspects:                                                                                                        \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 1. **Data Collection**: Llama 2 would have been trained on a large corpus of text data. This data is usually    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m sourced from a variety of publicly available datasets, including books, websites, and other text-rich sources.  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m The goal is to expose the model to diverse language patterns and knowledge.                                     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 2. **Preprocessing**: The collected data undergoes preprocessing, which includes cleaning the text, tokenizing  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m it into manageable pieces, and possibly filtering out any inappropriate content. This step ensures that the     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m data is in a suitable format for training.                                                                      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 3. **Model Architecture**: Llama 2 likely uses a transformer-based architecture, which is common in modern      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m language models. Transformers are effective at capturing long-range dependencies in text and are the backbone   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m of many state-of-the-art models.                                                                                \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 4. **Training Process**: The model is trained using a process called supervised learning, where it learns to    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m predict the next word in a sentence given the previous words. This involves adjusting the model's parameters to \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m minimize the difference between its predictions and the actual next words in the training data.                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 5. **Fine-Tuning**: After the initial training, Llama 2 may undergo fine-tuning on specific datasets to improve \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m its performance on particular tasks, such as chat or dialogue systems. Fine-tuning helps the model adapt to the \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m nuances of specific applications.                                                                               \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 6. **Evaluation and Iteration**: The model's performance is evaluated using various metrics, and the training   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m process may be iterated upon to improve accuracy, coherence, and relevance of the generated text.               \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m 7. **Ethical Considerations**: Throughout the training process, ethical considerations such as data privacy,    \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m bias mitigation, and content appropriateness are taken into account to ensure the model aligns with community   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m values and standards.                                                                                           \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m The snippets provided mention arXivLabs' values of openness, community, excellence, and user data privacy,      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m which suggests that any collaboration or project involving Llama 2 would adhere to these principles. However,   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m specific technical details about the training process of Llama 2 are not included in the text provided.         \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generated Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> The text provided appears to be a collection of snippets related to \"Llama 2: Open Foundation and Fine-Tuned    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> Chat Models\" and its association with arXivLabs. However, the text does not provide specific details on how     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> Llama 2 has been trained. To explain how Llama 2 has been trained, we would typically consider the following    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> aspects:                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 1. **Data Collection**: Llama 2 would have been trained on a large corpus of text data. This data is usually    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> sourced from a variety of publicly available datasets, including books, websites, and other text-rich sources.  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> The goal is to expose the model to diverse language patterns and knowledge.                                     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 2. **Preprocessing**: The collected data undergoes preprocessing, which includes cleaning the text, tokenizing  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> it into manageable pieces, and possibly filtering out any inappropriate content. This step ensures that the     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> data is in a suitable format for training.                                                                      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 3. **Model Architecture**: Llama 2 likely uses a transformer-based architecture, which is common in modern      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> language models. Transformers are effective at capturing long-range dependencies in text and are the backbone   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> of many state-of-the-art models.                                                                                <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 4. **Training Process**: The model is trained using a process called supervised learning, where it learns to    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> predict the next word in a sentence given the previous words. This involves adjusting the model's parameters to <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> minimize the difference between its predictions and the actual next words in the training data.                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 5. **Fine-Tuning**: After the initial training, Llama 2 may undergo fine-tuning on specific datasets to improve <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> its performance on particular tasks, such as chat or dialogue systems. Fine-tuning helps the model adapt to the <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> nuances of specific applications.                                                                               <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 6. **Evaluation and Iteration**: The model's performance is evaluated using various metrics, and the training   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> process may be iterated upon to improve accuracy, coherence, and relevance of the generated text.               <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> 7. **Ethical Considerations**: Throughout the training process, ethical considerations such as data privacy,    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> bias mitigation, and content appropriateness are taken into account to ensure the model aligns with community   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> values and standards.                                                                                           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> The snippets provided mention arXivLabs' values of openness, community, excellence, and user data privacy,      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> which suggests that any collaboration or project involving Llama 2 would adhere to these principles. However,   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> specific technical details about the training process of Llama 2 are not included in the text provided.         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a prompt where context from the Weaviate collection will be injected\n",
        "prompt = \"Explain how {text} has been trained.\"\n",
        "query = \"LLAMA\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query, limit=3, grouped_task=prompt, return_properties=[\"text\", \"title\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "\n",
        "console.print(\n",
        "    Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\")\n",
        ")\n",
        "console.print(\n",
        "    Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tGz49nfUegG"
      },
      "source": [
        "We can see that our RAG pipeline performs relatively well for simple queries, especially given the small size of the dataset. Scaling this method for converting a larger sample of PDFs would require more compute (GPUs) and a more advanced deployment of Weaviate (like Docker, Kubernetes, or Weaviate Cloud). For more information on available Weaviate configurations, check out the [documetation](https://weaviate.io/developers/weaviate/starter-guides/which-weaviate)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}